# é›†æˆæŒ‡å—ï¼šAudioProcessor + QwenOmniClient

æœ¬æŒ‡å—è¯´æ˜å¦‚ä½•å°†æ–°å®ç°çš„ `AudioProcessor` é›†æˆåˆ°ç°æœ‰çš„ `QwenOmniClient` ä¸­ï¼Œå®ç°è¿ç»­éŸ³é¢‘æµä¼ è¾“ã€‚

## å¿«é€Ÿé›†æˆ

### æ–¹æ¡ˆä¸€ï¼šç›´æ¥æ›¿æ¢ç°æœ‰å½•éŸ³é€»è¾‘ï¼ˆæ¨èï¼‰

åœ¨ `components/OmniChat.tsx` ä¸­æ›¿æ¢å½“å‰çš„å½•éŸ³å®ç°ï¼š

```typescript
import { AudioProcessor } from '../lib/audio/audio-processor';
import { arrayBufferToBase64 } from '../lib/utils';

export default function OmniChat() {
  const [isRecording, setIsRecording] = useState(false);
  const [audioLevel, setAudioLevel] = useState(0);
  
  const clientRef = useRef<QwenOmniClient | null>(null);
  const audioProcessorRef = useRef<AudioProcessor | null>(null);
  
  // åˆå§‹åŒ– AudioProcessor
  useEffect(() => {
    const processor = new AudioProcessor({
      sampleRate: 16000,
      channels: 1,
      chunkDurationMs: 20,
      
      // æ¯ 20ms è‡ªåŠ¨å‘é€ PCM16 éŸ³é¢‘åˆ°æœåŠ¡å™¨
      onAudioChunk: (buffer: ArrayBuffer) => {
        if (clientRef.current && isRecording) {
          const base64Audio = arrayBufferToBase64(buffer);
          
          // ç›´æ¥å‘é€åˆ° Qwen-Omni WebSocket
          clientRef.current.appendAudioBase64(base64Audio);
        }
      },
      
      // å®æ—¶éŸ³é¢‘ç”µå¹³
      onAudioLevel: (level: number) => {
        setAudioLevel(level);
      },
      
      // é”™è¯¯å¤„ç†
      onError: (error: string) => {
        console.error('Audio processor error:', error);
        setIsRecording(false);
      }
    });
    
    audioProcessorRef.current = processor;
    
    return () => {
      processor.dispose();
    };
  }, [isRecording]);
  
  // å¼€å§‹å½•éŸ³
  const startRecording = async () => {
    try {
      const processor = audioProcessorRef.current;
      if (!processor) return;
      
      // åˆå§‹åŒ–éº¦å…‹é£
      await processor.initialize();
      
      // å¼€å§‹è¿ç»­é‡‡é›†
      await processor.startCapture();
      
      setIsRecording(true);
      console.log('Continuous audio streaming started');
    } catch (error) {
      console.error('Failed to start recording:', error);
      
      if (error instanceof Error) {
        if (error.name === 'NotAllowedError') {
          alert('éº¦å…‹é£æƒé™è¢«æ‹’ç»ï¼Œè¯·å…è®¸è®¿é—®éº¦å…‹é£');
        } else if (error.name === 'NotFoundError') {
          alert('æœªæ‰¾åˆ°éº¦å…‹é£è®¾å¤‡');
        }
      }
    }
  };
  
  // åœæ­¢å½•éŸ³
  const stopRecording = () => {
    const processor = audioProcessorRef.current;
    if (!processor) return;
    
    processor.stopCapture();
    setIsRecording(false);
    setAudioLevel(0);
    
    // æäº¤éŸ³é¢‘ï¼ˆå‘Šè¯‰æœåŠ¡å™¨éŸ³é¢‘è¾“å…¥ç»“æŸï¼‰
    clientRef.current?.commit();
    
    console.log('Audio streaming stopped');
  };
  
  const toggleRecording = () => {
    if (isRecording) {
      stopRecording();
    } else {
      startRecording();
    }
  };
  
  // ... å…¶ä½™ UI ä»£ç ä¿æŒä¸å˜
}
```

### æ–¹æ¡ˆäºŒï¼šåˆ›å»ºä¸“ç”¨çš„éŸ³é¢‘æµç®¡ç†å™¨

åˆ›å»º `lib/audio/audio-stream-manager.ts`ï¼š

```typescript
import { AudioProcessor } from './audio-processor';
import { QwenOmniClient } from '../qwen-omni-client';
import { arrayBufferToBase64 } from '../utils';

export class AudioStreamManager {
  private processor: AudioProcessor;
  private client: QwenOmniClient;
  private isStreaming: boolean = false;
  
  constructor(
    client: QwenOmniClient,
    onAudioLevel?: (level: number) => void
  ) {
    this.client = client;
    
    this.processor = new AudioProcessor({
      sampleRate: 16000,
      channels: 1,
      chunkDurationMs: 20,
      
      onAudioChunk: (buffer: ArrayBuffer) => {
        if (this.isStreaming) {
          const base64Audio = arrayBufferToBase64(buffer);
          this.client.appendAudioBase64(base64Audio);
        }
      },
      
      onAudioLevel: (level: number) => {
        onAudioLevel?.(level);
      },
      
      onError: (error: string) => {
        console.error('Audio stream error:', error);
        this.stop();
      }
    });
  }
  
  async start(): Promise<void> {
    await this.processor.initialize();
    await this.processor.startCapture();
    this.isStreaming = true;
    console.log('Audio streaming started');
  }
  
  stop(): void {
    this.processor.stopCapture();
    this.isStreaming = false;
    this.client.commit();
    console.log('Audio streaming stopped');
  }
  
  isActive(): boolean {
    return this.isStreaming;
  }
  
  getAudioLevel(): number {
    return this.processor.getCurrentAudioLevel();
  }
  
  getStats() {
    return this.processor.getStats();
  }
  
  dispose(): void {
    this.stop();
    this.processor.dispose();
  }
}
```

ç„¶ååœ¨ç»„ä»¶ä¸­ä½¿ç”¨ï¼š

```typescript
import { AudioStreamManager } from '../lib/audio/audio-stream-manager';

export default function OmniChat() {
  const [audioLevel, setAudioLevel] = useState(0);
  const streamManagerRef = useRef<AudioStreamManager | null>(null);
  
  useEffect(() => {
    if (clientRef.current) {
      const manager = new AudioStreamManager(
        clientRef.current,
        (level) => setAudioLevel(level)
      );
      streamManagerRef.current = manager;
      
      return () => manager.dispose();
    }
  }, [clientRef.current]);
  
  const toggleRecording = async () => {
    const manager = streamManagerRef.current;
    if (!manager) return;
    
    if (manager.isActive()) {
      manager.stop();
    } else {
      await manager.start();
    }
  };
}
```

## QwenOmniClient å¢å¼º

ä¸ºäº†æ›´å¥½åœ°æ”¯æŒè¿ç»­éŸ³é¢‘æµï¼Œå¯ä»¥åœ¨ `QwenOmniClient` ä¸­æ·»åŠ ä¸€ä¸ªä¾¿æ·æ–¹æ³•ï¼š

```typescript
// åœ¨ lib/qwen-omni-client.ts ä¸­æ·»åŠ 

export class QwenOmniClient {
  // ... ç°æœ‰ä»£ç  ...
  
  /**
   * æ·»åŠ  base64 ç¼–ç çš„éŸ³é¢‘æ•°æ®ï¼ˆç”¨äºè¿ç»­æµï¼‰
   */
  appendAudioBase64(base64Audio: string): void {
    if (!this.isConnected() || !this.sessionId) {
      console.warn('Cannot append audio: not connected or no active session');
      return;
    }
    
    this.send({
      type: 'input_audio_buffer.append',
      audio: base64Audio
    });
  }
  
  /**
   * æäº¤éŸ³é¢‘ç¼“å†²åŒºï¼ˆæ ‡è®°éŸ³é¢‘è¾“å…¥ç»“æŸï¼‰
   */
  commit(): void {
    if (!this.isConnected() || !this.sessionId) {
      return;
    }
    
    this.send({
      type: 'input_audio_buffer.commit'
    });
  }
}
```

## å®Œæ•´ç¤ºä¾‹ï¼šPush-to-Talkï¼ˆæŒ‰ä½è¯´è¯ï¼‰

```typescript
export default function OmniChat() {
  const [isPushing, setIsPushing] = useState(false);
  const audioProcessorRef = useRef<AudioProcessor | null>(null);
  
  useEffect(() => {
    const processor = new AudioProcessor({
      onAudioChunk: (buffer: ArrayBuffer) => {
        if (isPushing && clientRef.current) {
          const base64Audio = arrayBufferToBase64(buffer);
          clientRef.current.appendAudioBase64(base64Audio);
        }
      }
    });
    
    audioProcessorRef.current = processor;
    
    // é¢„åˆå§‹åŒ–ï¼ˆé¿å…é¦–æ¬¡å»¶è¿Ÿï¼‰
    processor.initialize().catch(console.error);
    
    return () => processor.dispose();
  }, []);
  
  const handleMouseDown = async () => {
    const processor = audioProcessorRef.current;
    if (!processor) return;
    
    try {
      // ç¡®ä¿å·²åˆå§‹åŒ–
      if (!processor.isActive()) {
        await processor.startCapture();
      }
      
      setIsPushing(true);
      console.log('Push-to-talk: recording started');
    } catch (error) {
      console.error('Failed to start push-to-talk:', error);
    }
  };
  
  const handleMouseUp = () => {
    setIsPushing(false);
    
    // æäº¤å½“å‰éŸ³é¢‘æ®µ
    clientRef.current?.commit();
    
    console.log('Push-to-talk: recording stopped');
  };
  
  return (
    <button
      onMouseDown={handleMouseDown}
      onMouseUp={handleMouseUp}
      onMouseLeave={handleMouseUp}
      className={`px-6 py-6 rounded-full ${
        isPushing ? 'bg-red-500' : 'bg-blue-500'
      }`}
    >
      <Mic size={32} />
      {isPushing ? 'æ¾å¼€åœæ­¢' : 'æŒ‰ä½è¯´è¯'}
    </button>
  );
}
```

## å®Œæ•´ç¤ºä¾‹ï¼šè‡ªåŠ¨ VADï¼ˆè¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼‰

```typescript
export default function OmniChat() {
  const [isListening, setIsListening] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  
  useEffect(() => {
    const processor = new AudioProcessor({
      vadEnabled: true,        // å¯ç”¨ VAD
      vadThreshold: 0.01,      // æ ¹æ®ç¯å¢ƒè°ƒæ•´
      
      onAudioChunk: (buffer: ArrayBuffer) => {
        // åªæœ‰æ£€æµ‹åˆ°è¯­éŸ³æ—¶æ‰ä¼šè§¦å‘
        setIsSpeaking(true);
        
        const base64Audio = arrayBufferToBase64(buffer);
        clientRef.current?.appendAudioBase64(base64Audio);
        
        // é‡ç½®è¯´è¯çŠ¶æ€ï¼ˆç”¨äº UI æŒ‡ç¤ºï¼‰
        setTimeout(() => setIsSpeaking(false), 100);
      }
    });
    
    audioProcessorRef.current = processor;
    return () => processor.dispose();
  }, []);
  
  return (
    <div>
      <button onClick={() => toggleListening()}>
        {isListening ? 'åœæ­¢ç›‘å¬' : 'å¼€å§‹ç›‘å¬'}
      </button>
      
      {isListening && (
        <div className={`status ${isSpeaking ? 'speaking' : 'silent'}`}>
          {isSpeaking ? 'ğŸ—£ï¸ æ£€æµ‹åˆ°è¯­éŸ³' : 'ğŸ¤« ç­‰å¾…è¯­éŸ³...'}
        </div>
      )}
    </div>
  );
}
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. é¢„åŠ è½½éŸ³é¢‘å¤„ç†å™¨

```typescript
useEffect(() => {
  // åœ¨ç»„ä»¶æŒ‚è½½æ—¶é¢„åˆå§‹åŒ–ï¼Œé¿å…é¦–æ¬¡ç‚¹å‡»å»¶è¿Ÿ
  const processor = new AudioProcessor({ /* ... */ });
  processor.initialize().then(() => {
    console.log('Audio processor ready');
  });
  
  audioProcessorRef.current = processor;
  return () => processor.dispose();
}, []);
```

### 2. é”™è¯¯é‡è¯•æœºåˆ¶

```typescript
const startRecordingWithRetry = async (maxRetries = 3) => {
  for (let i = 0; i < maxRetries; i++) {
    try {
      await processor.initialize();
      await processor.startCapture();
      return;
    } catch (error) {
      console.warn(`Retry ${i + 1}/${maxRetries}:`, error);
      await new Promise(resolve => setTimeout(resolve, 1000));
    }
  }
  throw new Error('Failed to start recording after retries');
};
```

### 3. ç›‘æ§ç»Ÿè®¡ä¿¡æ¯

```typescript
useEffect(() => {
  const interval = setInterval(() => {
    const stats = audioProcessorRef.current?.getStats();
    if (stats) {
      console.log('Audio stats:', {
        bufferSize: stats.bufferSampleCount,
        duration: `${stats.bufferDuration * 1000}ms`,
        chunkSize: stats.chunkSize
      });
    }
  }, 5000);
  
  return () => clearInterval(interval);
}, []);
```

## æ•…éšœæ’æŸ¥

### é—®é¢˜ï¼šéŸ³é¢‘æ–­æ–­ç»­ç»­

**è§£å†³æ–¹æ¡ˆï¼š**
- æ£€æŸ¥ç½‘ç»œè¿æ¥
- å¢åŠ ç¼“å†²åŒºå¤§å°ï¼ˆè™½ç„¶ä¼šå¢åŠ å»¶è¿Ÿï¼‰
- ç¡®ä¿ä¸»çº¿ç¨‹ä¸è¢«é˜»å¡

### é—®é¢˜ï¼šéº¦å…‹é£æƒé™è¢«æ‹’

**è§£å†³æ–¹æ¡ˆï¼š**
```typescript
try {
  await processor.initialize();
} catch (error) {
  if (error.name === 'NotAllowedError') {
    // æ˜¾ç¤ºå‹å¥½çš„æç¤º
    showPermissionDialog();
  }
}
```

### é—®é¢˜ï¼šéŸ³é¢‘ç”µå¹³æ€»æ˜¯ 0

**è§£å†³æ–¹æ¡ˆï¼š**
- æ£€æŸ¥éº¦å…‹é£æ˜¯å¦é™éŸ³
- åœ¨ç³»ç»Ÿè®¾ç½®ä¸­æ£€æŸ¥éº¦å…‹é£éŸ³é‡
- éªŒè¯ `onAudioLevel` å›è°ƒæ˜¯å¦æ­£ç¡®è®¾ç½®

### é—®é¢˜ï¼šç¼–è¯‘é”™è¯¯ "ArrayBufferLike not assignable to ArrayBuffer"

**å·²è§£å†³ï¼š** ä»£ç å·²ä¿®å¤ï¼Œä½¿ç”¨æ­£ç¡®çš„ ArrayBuffer åˆ›å»ºæ–¹å¼

## æœ€ä½³å®è·µ

1. **æ€»æ˜¯å¤„ç†æƒé™é”™è¯¯**
2. **åœ¨ç»„ä»¶å¸è½½æ—¶è°ƒç”¨ `dispose()`**
3. **ä½¿ç”¨ useRef å­˜å‚¨ processor å®ä¾‹**
4. **ä¸è¦åœ¨ render å‡½æ•°ä¸­åˆ›å»ºæ–°çš„ processor**
5. **ç›‘æ§éŸ³é¢‘ç»Ÿè®¡ä¿¡æ¯ä»¥è¯Šæ–­é—®é¢˜**
6. **è€ƒè™‘æ·»åŠ ç”¨æˆ·åé¦ˆï¼ˆéŸ³é¢‘æ³¢å½¢ã€ç”µå¹³æŒ‡ç¤ºå™¨ï¼‰**

## æ€»ç»“

ä½¿ç”¨æ–°çš„ `AudioProcessor` å¯ä»¥å®ç°ï¼š
- âœ… è¿ç»­ã€ä½å»¶è¿Ÿçš„éŸ³é¢‘æµ
- âœ… è‡ªåŠ¨ PCM16 ç¼–ç 
- âœ… ç®€å•çš„ API æ¥å£
- âœ… å®Œæ•´çš„é”™è¯¯å¤„ç†
- âœ… å¯é€‰çš„ VAD æ”¯æŒ

åªéœ€å‡ è¡Œä»£ç å³å¯æ›¿æ¢ç°æœ‰çš„å½•éŸ³é€»è¾‘ï¼
